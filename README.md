# ModalPrompt

This repo is the official implementation of paper: **[ModalPrompt: Dual-Modality Guided Prompt for Continual Learning of Large Multimodal Models](https://arxiv.org/abs/2410.05849)**

> Modalprompt: Dual-modality guided prompt for continual learning of large multimodal models
>
> Fanhu Zeng, Fei Zhu, Haiyang Guo, Xu-Yao Zhang, Cheng-Lin Liu

[![arXiv](https://img.shields.io/badge/Arxiv-2410.05849-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2410.05849)

```bibtex
@article{zeng2024modalprompt,
  title={Modalprompt: Dual-modality guided prompt for continual learning of large multimodal models},
  author={Zeng, Fanhu and Zhu, Fei and Guo, Haiyang and Zhang, Xu-Yao and Liu, Cheng-Lin},
  journal={arXiv preprint arXiv:2410.05849},
  year={2024}
}
```

## Acknowledgememnt

The code is based on [LLaVA](https://github.com/haotian-liu/LLaVA). Thanks for these great works and open sourcing! 

If you find them helpful, please consider citing them as well. 
